import os
import pandas as pd
import src.visualization.plot_utils as plot_utils
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report
from src.config import EVALUATE_DIR, OUTPUT_PREDICT_DIR, OUTPUT_PREDICT_FILE, CONFUSION_DETAIL_FILE

def load_predict_data():
    """
    å¾ csv æª”æ¡ˆè©•ä¼°é æ¸¬çµæœ
    
    Returns:
        y_true (pd.Series): çœŸå¯¦æ¨™ç±¤
        y_pred (pd.Series): é æ¸¬æ¨™ç±¤
        y_proba (pd.Series): é æ¸¬æ©Ÿç‡
    """
    csv_path = os.path.join(OUTPUT_PREDICT_DIR, OUTPUT_PREDICT_FILE)
    df = pd.read_csv(csv_path)
    y_true = df['label']
    y_pred = df['predicted_label']
    y_proba = df['predicted_probability']
    return y_true, y_pred, y_proba

def evaluate_predictions(y_true, y_pred):
    """
    è¨ˆç®—è©•ä¼°æŒ‡æ¨™

    Args:
        y_true (pd.Series): çœŸå¯¦æ¨™ç±¤
        y_pred (pd.Series): é æ¸¬æ¨™ç±¤

    Returns:
        acc (float): æº–ç¢ºç‡
        prec (float): ç²¾ç¢ºç‡
        rec (float): å¬å›ç‡
        f1 (float): F1 Score
    """
    if y_true is None:
        print("âš ï¸ æ¸¬è©¦è³‡æ–™æ²’æœ‰ labelï¼Œç„¡æ³•è©•ä¼°")
        return

    acc = accuracy_score(y_true, y_pred)
    prec = precision_score(y_true, y_pred, zero_division=0)
    rec = recall_score(y_true, y_pred, zero_division=0)
    f1 = f1_score(y_true, y_pred, zero_division=0)

    print("\nğŸ“ˆ Evaluate Results:")
    print(f"Accuracy : {acc:.4f}")
    print(f"Precision: {prec:.4f}")
    print(f"Recall   : {rec:.4f}")
    print(f"F1 Score : {f1:.4f}")

    print("\nè©³ç´°åˆ†é¡å ±å‘Š:")
    print(classification_report(y_true, y_pred, zero_division=0))

def save_confusion_detail(y_true, y_pred):
    """
    å„²å­˜æ··æ·†çŸ©é™£è©³ç´°çµæœ
    """
    result = annotate_confusion_results(y_true, y_pred)
    
    os.makedirs(EVALUATE_DIR, exist_ok=True)
    result.to_csv(os.path.join(EVALUATE_DIR, CONFUSION_DETAIL_FILE), index=False)
    
    print(f"âœ… æ··æ·†çŸ©é™£è©³ç´°çµæœå·²å„²å­˜åˆ° {os.path.join(EVALUATE_DIR, CONFUSION_DETAIL_FILE)}")   

def annotate_confusion_results(y_true, y_pred):
    """
    å‚³å…¥ y_true, y_predï¼Œå›å‚³æ¯ç­†è³‡æ–™çš„ TP/FP/FN/TN åˆ†é¡æ¨™ç±¤èˆ‡çµ±è¨ˆå ±å‘Šã€‚
    """
    result = pd.DataFrame({
        'y_true': y_true,
        'y_pred': y_pred
    })

    def classify(row):
        if row['y_true'] == 1 and row['y_pred'] == 1:
            return 'TP'
        elif row['y_true'] == 1 and row['y_pred'] == 0:
            return 'FN'
        elif row['y_true'] == 0 and row['y_pred'] == 1:
            return 'FP'
        else:
            return 'TN'

    result['confusion'] = result.apply(classify, axis=1)

    # çµ±è¨ˆç¸½è¡¨
    stats = result['confusion'].value_counts().to_frame(name='count')
    stats['rate (%)'] = (stats['count'] / len(result) * 100).round(2)

    print("ğŸ“Š æ··æ·†çŸ©é™£åˆ†é¡çµæœçµ±è¨ˆï¼š")
    print(stats)


    return result

def main():
    """
    è©•ä¼°é æ¸¬çµæœ
    """
    try:
        # è¼‰å…¥é æ¸¬è³‡æ–™
        y_true, y_pred, y_proba = load_predict_data()

        # è©•ä¼°é æ¸¬çµæœ
        evaluate_predictions(y_true, y_pred)

        # å„²å­˜æ··æ·†çŸ©é™£è©³ç´°çµæœ
        save_confusion_detail(y_true, y_pred)

        # ç¹ªè£½ Precision / Recall / F1 vs Threshold
        plot_utils.plot_precision_recall_threshold(y_true, y_proba)
        
        # ç¹ªè£½ç‰¹å¾µé‡è¦æ€§
        plot_utils.plot_feature_importance_csv()
        
    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")


