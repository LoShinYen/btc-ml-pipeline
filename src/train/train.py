import os
import numpy as np
import pandas as pd
import xgboost as xgb
from sklearn.model_selection import train_test_split
from src.config import TRAIN_DIR, TRAIN_FILE , NON_FEATURE_COLS, SELECTED_FEATURES
from imblearn.over_sampling import SMOTE
from src.analysis.feature_analysis import save_feature_importance_csv
from src.train.timeseries_cv_search import cross_validate_xgboost_with_early_stopping

def load_train_data():
    """
    è®€å–å®Œæ•´çš„è¨“ç·´è³‡æ–™ DataFrame
    Returns:
        X (pd.DataFrame): ç‰¹å¾µé›†
        y (pd.Series): æ¨™ç±¤é›†
    """
    train_path = os.path.join(TRAIN_DIR, TRAIN_FILE)
    df = pd.read_csv(train_path)

    y = df['label']
    X = df[SELECTED_FEATURES]
    
    # feature_cols = [col for col in df.columns if col not in NON_FEATURE_COLS]
    print(f"âœ… åˆæ­¥è¼‰å…¥è³‡æ–™ï¼Œå…± {df.shape[0]} ç­†ï¼Œæ¬„ä½æ•¸é‡ {df.shape[1]} å€‹")
    # print(f"âœ… ç‰¹å¾µæ¬„ä½ï¼š{feature_cols}")
    # X = df[feature_cols]

    return X, y

def train_xgboost_with_cv(X, y):
    """
    ä½¿ç”¨ TimeSeriesSplit æ­é… GridSearchCV è‡ªå‹•èª¿åƒè¨“ç·´ XGBoostï¼Œä¸¦ä½¿ç”¨ early stoppingã€‚
    Args:
        X (pd.DataFrame): ç‰¹å¾µé›†
        y (pd.Series): æ¨™ç±¤é›†
    Returns:
        model: è¨“ç·´å®Œæˆçš„æœ€ä½³æ¨¡å‹
    """
    smote = SMOTE(random_state=42)
    X, y = smote.fit_resample(X, y)
    X['__index__'] = np.arange(len(X))
    X = X.sort_values('__index__').drop(columns='__index__')
    y = y.loc[X.index]
    
    base_params  = {
        'objective': 'binary:logistic',
        'eval_metric': 'aucpr',
        'scale_pos_weight': 1,
        'early_stopping_rounds': 50,
        'random_state': 42,
        # 'verbose': 100
    }

    # è¶…åƒæ•¸æœå°‹ç©ºé–“
    param_grid_dict  = {
        'max_depth': [5, 6],
        'learning_rate': [0.01, 0.05],
        'n_estimators': [500, 1000],
        'subsample': [0.8],
        'colsample_bytree': [0.8, 1.0]
    }
    
    print(f"âœ… é–‹å§‹é€²è¡Œ TimeSeriesSplit æœå°‹æœ€ä½³åƒæ•¸")

    best_params = cross_validate_xgboost_with_early_stopping(
        X, y,
        param_grid_dict,
        base_params=base_params,
        n_splits=5,
        metric='aucpr'
    )

    print(f"âœ… é–‹å§‹è¨“ç·´ final_model")
    
    # â­ å»ºç«‹ä¸€å€‹æ–°çš„ final_model
    final_model = xgb.XGBClassifier( **base_params, **best_params)

    # åˆ‡åˆ† Train/Val
    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, shuffle=False)
    
    final_model.fit(
        X_train,
        y_train,
        eval_set=[(X_val, y_val)],
        verbose=100
    )
    
    print('âœ… Final Model è¨“ç·´å®Œæˆ')

    return final_model

def train_xgboost_fast(X, y):
    from imblearn.over_sampling import SMOTE

    # âœ… æ™‚åºåˆ‡åˆ†ï¼ˆä¿ç•™å‰ 80%ï¼‰
    split_index = int(len(X) * 0.8)
    X_train, X_val = X.iloc[:split_index], X.iloc[split_index:]
    y_train, y_val = y.iloc[:split_index], y.iloc[split_index:]

    # âœ… æ¨¡å‹åƒæ•¸
    params = {
        'objective': 'binary:logistic',
        'eval_metric': 'aucpr',
        'max_depth': 7,
        'learning_rate': 0.03,
        'n_estimators': 1500,
        'subsample': 0.8,
        'colsample_bytree': 0.8,
        'scale_pos_weight': 1,
        'early_stopping_rounds': 50,
        'random_state': 42,
        "scale_pos_weight" : 12.6
    }

    model = xgb.XGBClassifier(**params)

    model.fit(
        X_train,
        y_train,
        eval_set=[(X_val, y_val)],  # âš ï¸ æœªå¹³è¡¡è³‡æ–™æ‰å¯ä½œç‚ºé©—è­‰æŒ‡æ¨™
        verbose=100
    )

    print("âœ… ç°¡æ˜“è¨“ç·´å®Œæˆï¼ˆä¿ç•™æ™‚åºã€è¨“ç·´é›†å¹³è¡¡ï¼‰")
    return model

def save_model_with_confirm(model):
    """
    å„²å­˜æ¨¡å‹

    Args:
        model (xgboost.XGBClassifier): è¨“ç·´å¥½çš„æ¨¡å‹
    """
    os.makedirs('./models/', exist_ok=True)
    base_path = './models/xgb_model.json'

    # if os.path.exists(base_path):
    #     answer = input(f"âš ï¸ æ¨¡å‹æª”æ¡ˆ {base_path} å·²å­˜åœ¨ï¼Œæ˜¯å¦è¦è¦†å¯«ï¼Ÿ(y/n): ").strip().lower()
    #     if answer != 'y':
    #         timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M')
    #         base_name = f"./models/xgb_model_{timestamp}.json"
    #         print(f"ğŸ“‚ è‡ªå‹•å„²å­˜æ–°æ¨¡å‹åï¼š{base_name}")
    #         model.save_model(base_name)
    #         print(f"âœ… æ¨¡å‹å·²æˆåŠŸå„²å­˜åˆ° {base_name}")
    #         return
        
    model.save_model(base_path)
    print(f"âœ… æ¨¡å‹å·²æˆåŠŸå„²å­˜åˆ° {base_path}")

def main():
    """
    ä¸»æµç¨‹
    """
    try:
        # Step 1: è¼‰å…¥è¨“ç·´è³‡æ–™  
        X,y = load_train_data()

        # Step 2: è¨“ç·´æ¨¡å‹
        # model = train_xgboost_with_cv(X,y)
        model = train_xgboost_fast(X,y)

        # Step 3: å„²å­˜æ¨¡å‹
        save_model_with_confirm(model)

        # Step 4: å„²å­˜ç‰¹å¾µé‡è¦æ€§
        save_feature_importance_csv(model, X.columns.tolist())
        
    except Exception as e:
        print(f"âŒ ç™¼ç”ŸéŒ¯èª¤: {e}")
